{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e9346e-0c04-436f-af80-8db18f77ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Analytics\n",
    "\n",
    "# 1. Extract Sample document and apply following document preprocessing methods:\n",
    "#    Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "# 2. Create representation of document by calculating Term Frequency and Inverse Document\n",
    "#    Frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffbf2397-e74c-4264-9c40-066f56d4365d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Anuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Anuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Anuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Anuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Anuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Optional for WordNet lemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90259dc-4f41-4d7e-9593-636054559e51",
   "metadata": {},
   "source": [
    "# Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8617d972-e8e1-4e7f-a084-ee110506d44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Words: ['AI', 'is', 'coined', 'in', 'the', 'year', '1956', ',', 'but', 'it', 'gained', 'popularity', 'recently', '.']\n",
      "Tokenized Sentences: ['AI is coined in the year 1956, but it gained popularity recently.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"AI is coined in the year 1956, but it gained popularity recently.\"\n",
    "\n",
    "# Word Tokenization\n",
    "word_tokens = word_tokenize(text)\n",
    "print(\"Tokenized Words:\", word_tokens)\n",
    "\n",
    "# Sentence Tokenization\n",
    "sent_tokens = sent_tokenize(text)\n",
    "print(\"Tokenized Sentences:\", sent_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca8f97a-54d9-45e3-83e0-e5d2c34989fb",
   "metadata": {},
   "source": [
    "#  POS Tagging (Part-of-Speech Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69a57ea8-5daf-4559-a6b6-99c6a30614c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('AI', 'NNP'), ('is', 'VBZ'), ('coined', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('year', 'NN'), ('1956', 'CD'), (',', ','), ('but', 'CC'), ('it', 'PRP'), ('gained', 'VBD'), ('popularity', 'NN'), ('recently', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "# POS Tagging\n",
    "pos_tags = pos_tag(word_tokens)\n",
    "print(\"POS Tags:\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bf3620-d964-43da-853c-656febd73ac1",
   "metadata": {},
   "source": [
    "#  Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87394602-8b77-4f5e-ad6b-40df89de9524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens after Stop Words Removal: ['AI', 'coined', 'year', '1956', ',', 'gained', 'popularity', 'recently', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stop Words Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "print(\"Tokens after Stop Words Removal:\", filtered_tokens)\n",
    "\n",
    "\n",
    "# Importing stopwords from nltk.corpus:\n",
    "# stopwords is a module in the nltk library that provides a list of common words (e.g., \"the\", \"is\", \"in\", \"and\") that are generally considered to be of little value in text analysis.\n",
    "\n",
    "# By importing stopwords from nltk.corpus, we can access a predefined list of these words. \n",
    "\n",
    "# stopwords.words('english') loads a list of English stop words.\n",
    "\n",
    "# We convert this list into a set using set(). A set is used here for faster look-up performance because checking membership in a set is faster than in a list.\n",
    "\n",
    "# word_tokens is a list of words that we obtained from the tokenization process.\n",
    "\n",
    "# We use a list comprehension to iterate through each word in word_tokens.\n",
    "\n",
    "# For each word:\n",
    "\n",
    "# word.lower() converts the word to lowercase to ensure that the comparison is case-insensitive. For example, \"The\" and \"the\" should be treated the same.\n",
    "\n",
    "# word.lower() not in stop_words checks if the lowercase version of the word is not in the set of stop words.\n",
    "\n",
    "# If the word is not in the stop words list, it gets added to the filtered_tokens list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7489466f-49df-4feb-a031-b4adfe5cdf7f",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "683f97ad-02ed-4f78-92ef-98f950c596f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens: ['ai', 'coin', 'year', '1956', ',', 'gain', 'popular', 'recent', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stemming the words\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70fbd1f-9c2c-45cf-bf38-5f5e9f3845ea",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b37166d8-173d-4e2b-b336-c4bd9424269a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens: ['AI', 'coined', 'year', '1956', ',', 'gained', 'popularity', 'recently', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatizing the words\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b365a3b1-d71b-4e6c-bf70-ed8501fe6e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ### **Term Frequency (TF)**\n",
    "\n",
    "# **Definition**: Term Frequency (TF) is a measure of how frequently a term appears in a document. It is calculated by dividing the number of times a term appears in the document by the total number of terms in the document.\n",
    "\n",
    "# $$\n",
    "# \\text{TF}(t) = \\frac{\\text{Number of times term t appears in a document}}{\\text{Total number of terms in the document}}\n",
    "# $$\n",
    "\n",
    "# **Example**:\n",
    "# For the sentence: \"AI is popular,\"\n",
    "\n",
    "# * \"AI\" appears 1 time, and the total number of words in the document is 3.\n",
    "# * TF for \"AI\" = $\\frac{1}{3} = 0.33$\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **Inverse Document Frequency (IDF)**\n",
    "\n",
    "# **Definition**: Inverse Document Frequency (IDF) measures how much information the word provides across the entire corpus. It is calculated by the logarithm of the total number of documents divided by the number of documents containing the term.\n",
    "\n",
    "# $$\n",
    "# \\text{IDF}(t) = \\log \\left( \\frac{N}{\\text{Number of documents containing t}} \\right)\n",
    "# $$\n",
    "\n",
    "# Where:\n",
    "\n",
    "# * $N$ is the total number of documents.\n",
    "\n",
    "# **Example**:\n",
    "# For a corpus of 3 documents:\n",
    "\n",
    "# * \"AI\" appears in 2 documents.\n",
    "# * IDF for \"AI\" = $\\log \\left( \\frac{3}{2} \\right) = 0.176$\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
    "\n",
    "# The TF-IDF score for a term in a document is the product of TF and IDF:\n",
    "\n",
    "# $$\n",
    "# \\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "# $$\n",
    "\n",
    "# This score indicates the importance of a word in a document relative to the entire corpus. The higher the TF-IDF score, the more relevant the word is in the given document.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **Formatted Example**:\n",
    "\n",
    "# Let's say we have the following 3 documents in a corpus:\n",
    "\n",
    "# 1. \"AI is coined in the year 1956, but it gained popularity recently.\"\n",
    "# 2. \"The field of AI has developed rapidly in recent years.\"\n",
    "# 3. \"AI applications are becoming widespread in many industries.\"\n",
    "\n",
    "# For each word in the document, the **TF** and **IDF** values are calculated, and then multiplied to get the **TF-IDF** score.\n",
    "# .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1bee83a-564a-4492-92b0-e374a3d3b7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency (TF) for each document:\n",
      "Document 1: {'AI': 0.125, 'is': 0.125, 'gaining': 0.125, 'popularity': 0.125, 'in': 0.125, 'the': 0.125, 'tech': 0.125, 'world.': 0.125}\n",
      "Document 2: {'The': 0.1, 'field': 0.1, 'of': 0.1, 'AI': 0.1, 'has': 0.1, 'rapidly': 0.1, 'developed': 0.1, 'in': 0.1, 'recent': 0.1, 'years.': 0.1}\n",
      "Document 3: {'AI': 0.08333333333333333, 'is': 0.08333333333333333, 'used': 0.08333333333333333, 'in': 0.08333333333333333, 'many': 0.08333333333333333, 'industries': 0.08333333333333333, 'such': 0.08333333333333333, 'as': 0.08333333333333333, 'healthcare,': 0.08333333333333333, 'finance,': 0.08333333333333333, 'and': 0.08333333333333333, 'transportation.': 0.08333333333333333}\n",
      "\n",
      "Inverse Document Frequency (IDF) for the corpus:\n",
      "{'gaining': 1.0986122886681098, 'is': 0.4054651081081644, 'popularity': 1.0986122886681098, 'tech': 1.0986122886681098, 'AI': 0.0, 'world.': 1.0986122886681098, 'the': 1.0986122886681098, 'in': 0.0, 'developed': 1.0986122886681098, 'has': 1.0986122886681098, 'The': 1.0986122886681098, 'recent': 1.0986122886681098, 'years.': 1.0986122886681098, 'of': 1.0986122886681098, 'field': 1.0986122886681098, 'rapidly': 1.0986122886681098, 'used': 1.0986122886681098, 'industries': 1.0986122886681098, 'such': 1.0986122886681098, 'healthcare,': 1.0986122886681098, 'finance,': 1.0986122886681098, 'as': 1.0986122886681098, 'transportation.': 1.0986122886681098, 'and': 1.0986122886681098, 'many': 1.0986122886681098}\n",
      "\n",
      "TF-IDF for each document:\n",
      "Document 1: {'AI': 0.0, 'is': 0.05068313851352055, 'gaining': 0.13732653608351372, 'popularity': 0.13732653608351372, 'in': 0.0, 'the': 0.13732653608351372, 'tech': 0.13732653608351372, 'world.': 0.13732653608351372}\n",
      "Document 2: {'The': 0.10986122886681099, 'field': 0.10986122886681099, 'of': 0.10986122886681099, 'AI': 0.0, 'has': 0.10986122886681099, 'rapidly': 0.10986122886681099, 'developed': 0.10986122886681099, 'in': 0.0, 'recent': 0.10986122886681099, 'years.': 0.10986122886681099}\n",
      "Document 3: {'AI': 0.0, 'is': 0.033788759009013694, 'used': 0.0915510240556758, 'in': 0.0, 'many': 0.0915510240556758, 'industries': 0.0915510240556758, 'such': 0.0915510240556758, 'as': 0.0915510240556758, 'healthcare,': 0.0915510240556758, 'finance,': 0.0915510240556758, 'and': 0.0915510240556758, 'transportation.': 0.0915510240556758}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Sample Documents\n",
    "corpus = [\n",
    "    \"AI is gaining popularity in the tech world.\",\n",
    "    \"The field of AI has rapidly developed in recent years.\",\n",
    "    \"AI is used in many industries such as healthcare, finance, and transportation.\"\n",
    "]\n",
    "\n",
    "# 1. Term Frequency (TF)\n",
    "def term_frequency(doc):\n",
    "    words = doc.split()\n",
    "    word_count = Counter(words)\n",
    "    tf = {word: count / len(words) for word, count in word_count.items()}\n",
    "    return tf\n",
    "\n",
    "# 2. Inverse Document Frequency (IDF)\n",
    "def inverse_document_frequency(corpus):\n",
    "    idf = {}\n",
    "    total_docs = len(corpus)\n",
    "    for doc in corpus:\n",
    "        for word in set(doc.split()):\n",
    "            idf[word] = idf.get(word, 0) + 1\n",
    "    return {word: math.log(total_docs / count) for word, count in idf.items()}\n",
    "\n",
    "# Calculate TF for each document\n",
    "tf_results = [term_frequency(doc) for doc in corpus]\n",
    "\n",
    "# Calculate IDF for the corpus\n",
    "idf_results = inverse_document_frequency(corpus)\n",
    "\n",
    "# Display TF and IDF\n",
    "print(\"Term Frequency (TF) for each document:\")\n",
    "for idx, doc_tf in enumerate(tf_results):\n",
    "    print(f\"Document {idx + 1}: {doc_tf}\")\n",
    "\n",
    "print(\"\\nInverse Document Frequency (IDF) for the corpus:\")\n",
    "print(idf_results)\n",
    "\n",
    "# Calculate and display TF-IDF for each document\n",
    "print(\"\\nTF-IDF for each document:\")\n",
    "for idx, doc_tf in enumerate(tf_results):\n",
    "    tfidf = {word: doc_tf[word] * idf_results.get(word, 0) for word in doc_tf}\n",
    "    print(f\"Document {idx + 1}: {tfidf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd4b53f-66e0-4757-9187-dd1530585bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
